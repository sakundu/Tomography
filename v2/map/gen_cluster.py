# Standard library imports
import os
import re
import sys
import time
from typing import Dict, IO, List, Tuple
import warnings

# Data processing and analysis
import numpy as np
import pandas as pd
from scipy.spatial import ConvexHull
from sklearn.cluster import KMeans
from sklearn.metrics import (
    calinski_harabasz_score,
    davies_bouldin_score,
    pairwise_distances,
    silhouette_score
)
from sklearn.neighbors import NearestNeighbors
from sklearn.preprocessing import StandardScaler
import hdbscan

# Graph and network related
import community as community_louvain
import igraph as ig
import leidenalg as la
import networkx as nx

# Visualization
import matplotlib.cm as cm
import matplotlib.pyplot as plt
from matplotlib.collections import PatchCollection
from matplotlib.patches import Polygon

# Suppress warnings
warnings.filterwarnings('ignore')

SEED = 42

def gen_ig_graph(edge_df:pd.DataFrame) -> ig.Graph:
    """
    Generate a igraph from edge file and node file
    """
    
    # Remove {}\\ from Source and Sink column
    edge_df['Source'] = edge_df['Source'].replace(
        to_replace=r'[{}\\]', 
        value='', 
        regex=True
    )
    
    edge_df['Sink'] = edge_df['Sink'].replace(
        to_replace=r'[{}\\]', 
        value='', 
        regex=True
    )
        
    tuples = [tuple(x) for x in edge_df[['Source', 'Sink', 'Weight']].values]
    G = ig.Graph.TupleList(tuples, directed=True, edge_attrs=['weight'])
    return G

def get_ig_subgraph(G:ig.Graph, node_list:List[str]) -> ig.Graph:
    """
    Get subgraph from igraph
    """
    # Get subgraph
    subG = G.subgraph(node_list)
    
    return subG

def gen_leiden_cluster(G:ig.Graph, node_df:pd.DataFrame, n_iteration:int = 100,
                       seed:int = SEED, is_weight:bool = True) -> pd.DataFrame:
    """
    Runs Leiden clustering on the input Graph and returns a dataframe where 
    each node is assigned a cluster_id
    """
    if is_weight:
        partition = la.find_partition(G, la.RBConfigurationVertexPartition,
                                  n_iterations=n_iteration, weights='weight',
                                  seed=seed)
    else:
        partition = la.find_partition(G, la.RBConfigurationVertexPartition,
                                  n_iterations=n_iteration, seed=seed)
        
    cluster_map = {}
    for cluster_id, nodes in enumerate(partition):
        for node in nodes:
            cluster_map[G.vs[node]['name']] = cluster_id

    cluster_df = pd.DataFrame(cluster_map.items(), 
                                columns=['Name', 'Cluster_id'])

    node_df = node_df.merge(cluster_df, on='Name', how='left')
    return node_df

def gen_inst_group_def(cluster_csv:str, cluster_def:str, design:str) -> None:
    '''
    Generate a def file that create cluster groups of instances.
    cluster_csv: csv file with cluster_id, instance name and type generated by
                run_louvain or run_leiden   
    cluster_def: def file that will be generated
    '''
    if not os.path.exists(cluster_csv):
        print(f"Error: {cluster_csv} does not exist")
        exit()
    
    cluster_def_dir = os.path.dirname(cluster_def)
    # Create directory if specified and non-empty
    if cluster_def_dir:
        if not os.path.exists(cluster_def_dir):
            print(f"Warning: {cluster_def_dir} does not exist")
            print(f"Creating {cluster_def_dir}")
            os.makedirs(cluster_def_dir)
        
    cluster_df = pd.read_csv(cluster_csv)
    cluster_df = cluster_df.sort_values(by=['Cluster_id'])
    instance_df = cluster_df
    cluster_ids = instance_df['Cluster_id'].unique().tolist()
    
    fp = open(cluster_def, 'w')
    fp.write(f"VERSION 5.8 ;\n")
    fp.write(f"DESIGN {design} ;\n")
    fp.write(f"GROUPS {len(cluster_ids)} ;\n")
    for cluster_id in sorted(cluster_ids):
        fp.write(f"- cluster_{cluster_id} \n")
        for instance in instance_df[instance_df['Cluster_id'] ==\
                cluster_id]['Name']:
            instance = re.sub(r'^\{|\}$', '', instance)
            fp.write(f"    {instance}\n")
        fp.write(f";\n")
    
    fp.write(f"END GROUPS\n")
    fp.write(f"END DESIGN\n")
    fp.close()


def update_edge_length(edge_df:pd.DataFrame, node_df:pd.DataFrame) -> pd.DataFrame:
    """
    Update edge length based on the distance between the two nodes
    """
    # Get the coordinates of the nodes
    node_df['PT_X'] = node_df['PT_X'].astype(float)
    node_df['PT_Y'] = node_df['PT_Y'].astype(float)
    
    temp_node_df = node_df[['Instance', 'PT_X', 'PT_Y']]
    temp_node_df = temp_node_df.rename(columns={'Instance': 'Source', 
                                                'PT_X': 'Source_X', 
                                                'PT_Y': 'Source_Y'})
    
    ## Merge the edge_df with temp_node_df to get the coordinates of the source node
    edge_df = edge_df.merge(temp_node_df, on='Source', how='left')
    
    temp_node_df = node_df[['Instance', 'PT_X', 'PT_Y']]
    temp_node_df = temp_node_df.rename(columns={'Instance': 'Sink', 
                                                'PT_X': 'Sink_X', 
                                                'PT_Y': 'Sink_Y'})
    ## Merge the edge_df with temp_node_df to get the coordinates of the sink node
    edge_df = edge_df.merge(temp_node_df, on='Sink', how='left')
    ## Calculate the distance between the two nodes
    edge_df['Length'] = np.sqrt((edge_df['Source_X'] - edge_df['Sink_X'])**2 + 
                                (edge_df['Source_Y'] - edge_df['Sink_Y'])**2)
    ## Drop the Source_X, Source_Y, Sink_X, Sink_Y columns
    edge_df = edge_df.drop(columns=['Source_X', 'Source_Y', 'Sink_X', 'Sink_Y'])
    ## Remove the edges with length 0
    edge_df = edge_df[edge_df['Length'] > 0]
    
    edge_df['Weight'] = 1/edge_df['Length']
    
    ## Drop the Length column
    edge_df = edge_df.drop(columns=['Length'])
    
    return edge_df

def update_node_df(node_df:pd.DataFrame) -> pd.DataFrame:
    """
    Update the node_df to remove the {} and \ from the Name column
    """
    ##  First get the unique instances count
    unique_instances = node_df['Instance'].nunique()
    print(f"Unique instances: {unique_instances}")

    ## Now Drop the item for which "Slack" is INFINITY
    node_df = node_df[node_df['Slack'] != float('inf')]
    node_df = node_df[node_df['Slack'] != float('-inf')]

    ## Drop the duplicate instances
    node_df = node_df.drop_duplicates(subset=['Instance'])

    ## Sort based on Slack
    node_df = node_df.sort_values(by='Slack', ascending=True)

    ## Only consider the top 35% of the instances
    top_35_percent = int(len(node_df) * 0.3)
    node_df = node_df.head(top_35_percent)
    
    return node_df

def run_leiden_cluster_helper(node_file:str, edge_file:str, node_file2:str = None) -> None:
    
    node_df = pd.read_csv(node_file)
    edge_df = pd.read_csv(edge_file)
    
    edge_df = update_edge_length(edge_df, node_df)
    node_df = update_node_df(node_df)
    node_df = node_df.rename(columns={'Instance': 'Name'})
    G = gen_ig_graph(edge_df)
    node_list = node_df['Name'].tolist()
    if node_file2 is not None:
        node_df2 = pd.read_csv(node_file2)
        ## Find all the 'Name' of node_df2 that are in node_df
        node_df2 = node_df2[node_df2['Name'].isin(node_df['Name'])]
        node_list = node_df2['Name'].tolist()
    
    # Filter node_list to only include nodes that exist in the graph
    existing_nodes = set(G.vs['name'])
    node_list = [node for node in node_list if node in existing_nodes]
    
    subG = get_ig_subgraph(G, node_list)
    node_df = gen_leiden_cluster(subG, node_df, n_iteration=100, seed=SEED)
    
    ## Edge file replace .csv with _cluster.csv
    cluster_file = re.sub(r'\.csv$', '_cluster.csv', edge_file)
    
    node_df.to_csv(cluster_file, index=False)
    print(f"Cluster file: {cluster_file}")
    
    return

def run_leiden_cluster(design:str, data_dir:str) -> None:
    """
    Run leiden clustering on the input design and data directory
    """
    node_file = f"{data_dir}/{design}_slack_pt.csv"
    edge_file = f"{data_dir}/{design}_edge_slack_length.csv"
    
    run_leiden_cluster_helper(node_file, edge_file)
    
    cluster_file = re.sub(r'\.csv$', '_cluster.csv', edge_file)
    cluster_def = f"{data_dir}/{design}_leiden_cluster.def"
    gen_inst_group_def(cluster_file, cluster_def, design)
    print(f"Cluster def file: {cluster_def}")
    
    ## Path based
    node_file2 = f"{data_dir}/{design}_nodes.csv"
    run_leiden_cluster_helper(node_file, edge_file, node_file2)
    cluster_def = f"{data_dir}/{design}_leiden_cluster_path.def"
    gen_inst_group_def(cluster_file, cluster_def, design)
    print(f"Cluster def file: {cluster_def}")
    return

def cluster_with_hdbscan(df, min_cluster_size=200, max_cluster_size=5000):
    """
    Performs balanced HDBSCAN clustering with controlled cluster size bounds.
    
    Parameters:
    df (pandas.DataFrame): Dataframe with columns 'Instance', 'PT_X', and 'PT_Y'
    min_cluster_size (int): Minimum desired size for each cluster
    max_cluster_size (int): Maximum allowed size for each cluster
    
    Returns:
    pandas.DataFrame: Dataframe with columns 'Instance' and 'Cluster_ID'
    """
    # Extract points for clustering
    points = df[['PT_X', 'PT_Y']].values
    
    # Scale the data
    scaler = StandardScaler()
    points_scaled = scaler.fit_transform(points)
    
    # Determine initial HDBSCAN parameters based on data size
    data_size = len(points)
    
    # For small datasets, use higher min_cluster_size to avoid too many clusters
    hdbscan_min_size = max(20, min(100, int(data_size / 50)))
    
    # Run initial HDBSCAN with moderate parameters
    clusterer = hdbscan.HDBSCAN(
        min_cluster_size=hdbscan_min_size,
        min_samples=5,
        cluster_selection_epsilon=0.5,
        metric='euclidean'
    )
    labels = clusterer.fit_predict(points_scaled)
    
    # Count initial clusters and noise
    unique_labels = set(labels)
    n_clusters = len(unique_labels) - (1 if -1 in labels else 0)
    num_noise = np.sum(labels == -1)
    
    print(f"Initial clustering: {n_clusters} clusters, {num_noise} noise points ({100*num_noise/len(labels):.1f}% of data)")
    
    # Calculate cluster sizes
    cluster_sizes = {}
    for label in unique_labels:
        if label != -1:  # Skip noise points
            size = np.sum(labels == label)
            cluster_sizes[label] = size
            
    # Identify clusters that need splitting (too large) and those that need merging (too small)
    large_clusters = [(label, size) for label, size in cluster_sizes.items() if size > max_cluster_size]
    small_clusters = [(label, size) for label, size in cluster_sizes.items() if size < min_cluster_size]
    
    print(f"Found {len(large_clusters)} clusters larger than {max_cluster_size} points")
    print(f"Found {len(small_clusters)} clusters smaller than {min_cluster_size} points")
    
    # If we have large clusters or small clusters, adjust the clustering
    if large_clusters or small_clusters or num_noise > 0:
        print("Balancing cluster sizes...")
        
        # Start with new labels
        balanced_labels = np.zeros(len(points), dtype=int) - 1  # All noise initially
        next_label = 0
        
        # STEP 1: Handle large clusters by splitting them
        for label, size in sorted(cluster_sizes.items(), key=lambda x: x[1], reverse=True):
            if size > max_cluster_size:
                # Get points in this large cluster
                mask = (labels == label)
                cluster_points = points_scaled[mask]
                cluster_indices = np.where(mask)[0]
                
                # Calculate how many subclusters needed
                n_subclusters = max(1, int(np.ceil(size / (max_cluster_size * 0.8))))
                
                # Apply KMeans to split the cluster
                kmeans = KMeans(n_clusters=n_subclusters, random_state=42)
                sub_labels = kmeans.fit_predict(cluster_points)
                
                # Assign subclusters
                for sub_label in range(n_subclusters):
                    sub_mask = sub_labels == sub_label
                    sub_indices = cluster_indices[sub_mask]
                    
                    # Only create a new cluster if it has enough points
                    if len(sub_indices) >= min_cluster_size:
                        balanced_labels[sub_indices] = next_label
                        next_label += 1
                    else:
                        # Mark smaller splits for later assignment
                        balanced_labels[sub_indices] = -2  # Temporary marker
            else:
                # Keep medium-sized clusters as they are
                if size >= min_cluster_size:
                    mask = (labels == label)
                    balanced_labels[mask] = next_label
                    next_label += 1
                else:
                    # Mark small clusters for merging
                    mask = (labels == label)
                    balanced_labels[mask] = -2  # Temporary marker
                    
        # STEP 2: Assign noise points and small clusters to nearest larger cluster
        unassigned_mask = (balanced_labels < 0)
        unassigned_indices = np.where(unassigned_mask)[0]
        
        if np.any(~unassigned_mask):  # If we have assigned points
            # Find valid clusters
            assigned_mask = ~unassigned_mask
            assigned_points = points_scaled[assigned_mask]
            assigned_labels = balanced_labels[assigned_mask]
            
            # Get unique valid labels
            valid_labels = np.unique(assigned_labels)
            
            # Find centroids of valid clusters
            centroids = {}
            for vlabel in valid_labels:
                centroid_mask = (assigned_labels == vlabel)
                centroids[vlabel] = np.mean(assigned_points[centroid_mask], axis=0)
            
            # Use nearest centroid for assignment
            for idx in unassigned_indices:
                point = points_scaled[idx].reshape(1, -1)
                
                # Find nearest centroid
                min_dist = float('inf')
                nearest_label = 0
                
                for vlabel, centroid in centroids.items():
                    dist = np.linalg.norm(point - centroid)
                    if dist < min_dist:
                        min_dist = dist
                        nearest_label = vlabel
                        
                balanced_labels[idx] = nearest_label
        else:
            # If all points are unassigned, create balanced clusters using KMeans
            target_clusters = max(1, min(20, int(data_size / 2000)))
            kmeans = KMeans(n_clusters=target_clusters, random_state=42)
            balanced_labels = kmeans.fit_predict(points_scaled)
    
        # Update cluster count
        unique_labels = set(balanced_labels)
        n_clusters = len(unique_labels)
        
        labels = balanced_labels
    
    # Final balancing step if needed
    if n_clusters < 2 and len(points) > min_cluster_size*3:
        # If we ended up with too few clusters, force split using KMeans
        target_clusters = max(3, min(15, int(data_size / 3000)))
        print(f"Too few clusters, using KMeans with {target_clusters} clusters")
        kmeans = KMeans(n_clusters=target_clusters, random_state=42)
        labels = kmeans.fit_predict(points_scaled)
    
    # Update cluster count
    unique_labels = set(labels)
    n_clusters = len(unique_labels)
    
    print(f"Final result: {n_clusters} clusters")
    
    # Report cluster statistics
    sizes = [np.sum(labels == label) for label in unique_labels]
    print(f"Cluster size statistics:")
    print(f"  - Min: {min(sizes)}")
    print(f"  - Max: {max(sizes)}")
    print(f"  - Avg: {np.mean(sizes):.1f}")
    print(f"  - Median: {np.median(sizes):.1f}")
    
    # Create result dataframe
    result_df = pd.DataFrame({
        'Name': df['Instance'],
        'Cluster_id': labels
    })
    
    return result_df

def run_hdbscan_cluster(data_dir:str, design:str) -> None:
    """
    Run HDBSCAN clustering on the input node file and design
    """
    node_file = f"{data_dir}/{design}_slack_pt.csv"
    node_df = pd.read_csv(node_file)
    node_df = update_node_df(node_df)
    
    # Perform HDBSCAN clustering
    cluster_df = cluster_with_hdbscan(node_df)
    
    # Save the cluster results
    cluster_file = re.sub(r'\.csv$', '_hdbscan_cluster.csv', node_file)
    cluster_df.to_csv(cluster_file, index=False)
    
    # Generate the def file
    cluster_def = f"{os.path.dirname(node_file)}/{design}_hdbscan_cluster.def"
    gen_inst_group_def(cluster_file, cluster_def, design)
    
    print(f"Cluster def file: {cluster_def}")
    
    node_file2 = f"{data_dir}/{design}_nodes.csv"
    node_df2 = pd.read_csv(node_file2)
    node_df = pd.read_csv(node_file)
    node_df = node_df.rename(columns={'Instance': 'Name'})
    node_df = node_df[['Name', 'PT_X', 'PT_Y']]
    node_df2 = node_df2[node_df2['Name'].isin(node_df['Name'])]
    ## Merge the node_df and node_df2 to get the coordinates of the nodes
    node_df2 = node_df2.merge(node_df, on='Name', how='left')
    node_df2 = node_df2[['Name', 'PT_X', 'PT_Y']]
    ## Rename "Name" to "Instance"
    node_df2 = node_df2.rename(columns={'Name': 'Instance'})
    print(f"Node df2: {node_df2.shape}")
    cluster_df = cluster_with_hdbscan(node_df2)
    print(f"Cluster df: {cluster_df.shape}")
    cluster_file = re.sub(r'\.csv$', '_hdbscan_cluster_path.csv', node_file)
    cluster_df.to_csv(cluster_file, index=False)
    cluster_def = f"{os.path.dirname(node_file)}/{design}_hdbscan_cluster_path.def"
    gen_inst_group_def(cluster_file, cluster_def, design)
    print(f"Cluster def file: {cluster_def}")
    return

if __name__ == "__main__":
    design = sys.argv[1]
    data_dir = sys.argv[2]
    run_leiden_cluster(design, data_dir)
    run_hdbscan_cluster(data_dir, design)
    print("Clustering completed.")
